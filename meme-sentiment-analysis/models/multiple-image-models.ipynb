{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação utilizando somente imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1917,
     "status": "ok",
     "timestamp": 1657489510982,
     "user": {
      "displayName": "Vitor Hugo de Sousa Ferreira",
      "userId": "07661415699331402133"
     },
     "user_tz": 180
    },
    "id": "0YUF9DN_zPxS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from keras.utils.layer_utils import count_params\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../config.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        configs = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN_DS_PROCESSED = str(configs['ROOT_DIR'] + configs['PATH_TO_TRAIN_DS_PROCESSED'])\n",
    "PATH_TO_WEIGHTS            = str(configs['ROOT_DIR'] + configs['PATH_TO_WEIGHTS'])\n",
    "PATH_TO_IMAGES             = str(configs['ROOT_DIR'] + configs['PATH_TO_IMAGES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657489518331,
     "user": {
      "displayName": "Vitor Hugo de Sousa Ferreira",
      "userId": "07661415699331402133"
     },
     "user_tz": 180
    },
    "id": "3LQZtcVmJeMc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>text</th>\n",
       "      <th>humour</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>offensive</th>\n",
       "      <th>motivational</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_1415.jpeg</td>\n",
       "      <td>if you want to view paradis simpli look around...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_6460.png</td>\n",
       "      <td>if i had a brick for everi lie hillari told i ...</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>general</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_2303.png</td>\n",
       "      <td>that thing over there can i eat that</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_2417.png</td>\n",
       "      <td>my dad point to liam and said when did david b...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>general</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_11.jpg</td>\n",
       "      <td>probabl the first man to do year challeng chen...</td>\n",
       "      <td>funny</td>\n",
       "      <td>general</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_name                                               text  \\\n",
       "0  image_1415.jpeg  if you want to view paradis simpli look around...   \n",
       "1   image_6460.png  if i had a brick for everi lie hillari told i ...   \n",
       "2   image_2303.png               that thing over there can i eat that   \n",
       "3   image_2417.png  my dad point to liam and said when did david b...   \n",
       "4     image_11.jpg  probabl the first man to do year challeng chen...   \n",
       "\n",
       "       humour        sarcasm       offensive  motivational         target  \n",
       "0   not_funny  not_sarcastic   not_offensive  motivational       positive  \n",
       "1   hilarious        general  very_offensive  motivational       positive  \n",
       "2  very_funny        general   not_offensive  motivational  very_positive  \n",
       "3   not_funny        general  very_offensive  motivational        neutral  \n",
       "4       funny        general  very_offensive  motivational       negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned = pd.read_csv(PATH_TO_TRAIN_DS_PROCESSED)\n",
    "df_train_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "W37WDUpPDqvy",
    "outputId": "17223c44-4b9f-4244-c1fd-4f4e5617b791"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>humour</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>offensive</th>\n",
       "      <th>motivational</th>\n",
       "      <th>label</th>\n",
       "      <th>text_corrected_cleaned_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_1415.jpeg</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "      <td>if you want to view paradis simpli look around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_6460.png</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>general</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "      <td>if i had a brick for everi lie hillari told i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_2303.png</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "      <td>that thing over there can i eat that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_2417.png</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>general</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>neutral</td>\n",
       "      <td>my dad point to liam and said when did david b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_11.jpg</td>\n",
       "      <td>funny</td>\n",
       "      <td>general</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>negative</td>\n",
       "      <td>probabl the first man to do year challeng chen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_name      humour        sarcasm       offensive  motivational  \\\n",
       "0  image_1415.jpeg   not_funny  not_sarcastic   not_offensive  motivational   \n",
       "1   image_6460.png   hilarious        general  very_offensive  motivational   \n",
       "2   image_2303.png  very_funny        general   not_offensive  motivational   \n",
       "3   image_2417.png   not_funny        general  very_offensive  motivational   \n",
       "4     image_11.jpg       funny        general  very_offensive  motivational   \n",
       "\n",
       "      label                     text_corrected_cleaned_stemmed  \n",
       "0  positive  if you want to view paradis simpli look around...  \n",
       "1  positive  if i had a brick for everi lie hillari told i ...  \n",
       "2  positive               that thing over there can i eat that  \n",
       "3   neutral  my dad point to liam and said when did david b...  \n",
       "4  negative  probabl the first man to do year challeng chen...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned.replace(\"very_positive\", \n",
    "           \"positive\", \n",
    "           inplace=True)\n",
    "df_train_cleaned.replace(\"very_negative\", \n",
    "           \"negative\", \n",
    "           inplace=True)       \n",
    "df_train_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hEt4Z5VZDqvy",
    "outputId": "1eccc3c0-2df9-4190-8b28-c3f81bf9a252"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Frequencia das classes no conjunto de treinamento pré-divisão (5453 amostras)'}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Frequencia das classes no conjunto de treinamento pré-divisão (%i amostras)' % len(df_train_cleaned))\n",
    "labels, counts = np.unique(df_train_cleaned['target'], return_counts=True)\n",
    "sns.barplot(labels, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão do conjunto de dados (treino/validação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 356.97it/s]\n",
      "C:\\Users\\Vitor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Vitor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Vitor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Frequencia das classes no conjunto de validação (1090 amostras)'}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_frac = 0.8\n",
    "id_col = 'image_name'\n",
    "df_train_val = get_train_val_split(train_frac, df_train_cleaned, id_col)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_train_val['label_encoder'] = le.fit_transform(df_train_val[['target']])\n",
    "\n",
    "df_sep_train = df_train_val[df_train_val.split=='train'].drop(columns='split')\n",
    "df_sep_val = df_train_val[df_train_val.split=='val'].drop(columns='split')\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(121)\n",
    "plt.title('Frequencia das classes no conjunto de treinamento (%i amostras)' % len(df_sep_train))\n",
    "labels, counts = np.unique(df_sep_train['target'], return_counts=True)\n",
    "sns.barplot(labels, counts)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(121)\n",
    "plt.title('Frequencia das classes no conjunto de validação (%i amostras)' % len(df_sep_val))\n",
    "labels, counts = np.unique(df_sep_val['target'], return_counts=True)\n",
    "sns.barplot(labels, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo dos pesos das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: negative\t| Weight: 3.6177446102819237\n",
      "Class: neutral\t| Weight: 1.0693627450980392\n",
      "Class: positive\t| Weight: 0.5591439190055107\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                     classes=np.unique(df_sep_train['target']), \n",
    "                                     y=df_sep_train['target']\n",
    "                                    )\n",
    "train_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "for i, label in enumerate(np.unique(df_sep_train['target'])):\n",
    "    print('Class: '+label+'\\t| Weight: '+str(class_weights[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████▏                               | 2840/4363 [00:09<00:04, 314.91it/s]C:\\Users\\Vitor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:973: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 4363/4363 [00:15<00:00, 280.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1090/1090 [00:04<00:00, 250.19it/s]\n"
     ]
    }
   ],
   "source": [
    "#Carregando imagens do treino\n",
    "width = 100\n",
    "height = 100\n",
    "X_train = []\n",
    "X_train_path = []\n",
    "for i in tqdm(df_sep_train['image_name']):\n",
    "    path = PATH_TO_IMAGES + i\n",
    "    img = image.load_img(path,target_size=(width,height,3))\n",
    "    img = image.img_to_array(img)\n",
    "    # img = img/255.0\n",
    "    X_train.append(img)\n",
    "    X_train_path.append(path)\n",
    "\n",
    "#Carregando imagens da validação\n",
    "X_val = []\n",
    "X_val_path = []\n",
    "for i in tqdm(df_sep_val['image_name']):\n",
    "    path = PATH_TO_IMAGES + i\n",
    "    img = image.load_img(path,target_size=(width,height,3))\n",
    "    img = image.img_to_array(img)\n",
    "    # img = img/255.0\n",
    "    X_val.append(img)\n",
    "    X_val_path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algumas transformações\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "Y_train = df_sep_train.iloc[:,-1].astype('category')\n",
    "Y_train_code = Y_train.cat.codes\n",
    "Y_val = df_sep_val.iloc[:,-1].astype('category')\n",
    "Y_val_code=Y_val.cat.codes\n",
    "n_classes=len(np.unique(df_sep_train['target']))\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, n_classes)\n",
    "Y_val = tf.keras.utils.to_categorical(Y_val, n_classes)\n",
    "\n",
    "#Criando o gerador para as transformações\n",
    "#Aqui, após alguns testes, encontrei que as melhores aumentações são a mudança de brilho e mudanças nos canais da imagem.\n",
    "#Faz sentido pq elas não descaracterizam o meme.\n",
    "datagen_image = ImageDataGenerator(brightness_range=[0.7,1.5],preprocessing_function=tf.keras.applications.resnet50.preprocess_input,channel_shift_range=20)\n",
    "\n",
    "train_batches = datagen_image.flow(X_train, np.array(Y_train), shuffle=False, batch_size=64)\n",
    "val_batches = datagen_image.flow(X_val, np.array(Y_val), shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_frozen_network(originalNN, numTrainableLayers, numClasses):\n",
    "    # Congela camadas pré-treinadas menos últimas 4 que serão feitas o fine tunning\n",
    "    for layer in originalNN.layers[:-numTrainableLayers]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Inclui camada de avg pooling e densa no final\n",
    "    newNN = tf.keras.Sequential([originalNN,\n",
    "                                 layers.GlobalAveragePooling2D(),\n",
    "                                 layers.Dense(numClasses, activation='softmax')\n",
    "                                ])\n",
    "\n",
    "    # Compila modelo com otimizador sgd\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, decay=0.001)\n",
    "    newNN.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy']\n",
    "                 )\n",
    "    print('Numero de parametros treinaveis: '+str(count_params(newNN.trainable_weights)))\n",
    "    \n",
    "    return newNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de parametros treinaveis: 1060867\n",
      "Epoch 1/500\n",
      "69/69 [==============================] - 55s 770ms/step - loss: 4.4350 - accuracy: 0.3094 - val_loss: 3.8075 - val_accuracy: 0.5945\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 53s 767ms/step - loss: 4.8868 - accuracy: 0.3165 - val_loss: 1.6494 - val_accuracy: 0.5560\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 53s 763ms/step - loss: 3.0178 - accuracy: 0.3486 - val_loss: 1.4420 - val_accuracy: 0.3394\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 2.9974 - accuracy: 0.3424 - val_loss: 2.8335 - val_accuracy: 0.2523\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 3.4146 - accuracy: 0.3333 - val_loss: 1.4050 - val_accuracy: 0.5679\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 53s 769ms/step - loss: 2.4622 - accuracy: 0.2925 - val_loss: 1.1993 - val_accuracy: 0.5688\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 53s 763ms/step - loss: 2.4411 - accuracy: 0.3718 - val_loss: 1.1808 - val_accuracy: 0.5202\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 53s 766ms/step - loss: 3.2626 - accuracy: 0.4034 - val_loss: 1.5938 - val_accuracy: 0.4505\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 53s 768ms/step - loss: 2.5253 - accuracy: 0.4268 - val_loss: 2.0329 - val_accuracy: 0.5385\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 3.6224 - accuracy: 0.3651 - val_loss: 2.0871 - val_accuracy: 0.5972\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 1.4236 - accuracy: 0.4939 - val_loss: 1.3251 - val_accuracy: 0.3404\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 53s 765ms/step - loss: 1.2870 - accuracy: 0.4577 - val_loss: 1.5875 - val_accuracy: 0.2853\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 53s 765ms/step - loss: 1.1848 - accuracy: 0.5047 - val_loss: 2.4334 - val_accuracy: 0.2009\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 53s 765ms/step - loss: 1.4039 - accuracy: 0.3979 - val_loss: 1.4499 - val_accuracy: 0.3679\n",
      "Epoch 15/500\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 1.4098 - accuracy: 0.4566 - val_loss: 1.5777 - val_accuracy: 0.5917\n",
      "Epoch 16/500\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 0.9894 - accuracy: 0.4903 - val_loss: 1.2816 - val_accuracy: 0.4046\n",
      "Epoch 17/500\n",
      "69/69 [==============================] - 53s 769ms/step - loss: 1.2640 - accuracy: 0.5198 - val_loss: 1.1918 - val_accuracy: 0.3982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d5e4e81f0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Carregamento do modelo pré-treinado sem as camadas densas\n",
    "model_ResNet50 = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(width,height,3))\n",
    "# Cria modelo para posterior fine tunning\n",
    "model_ResNet50_frozen = define_frozen_network(model_ResNet50, 4, n_classes)\n",
    "\n",
    "# Treina modelo\n",
    "model_ResNet50_frozen.fit(train_batches, \n",
    "                          epochs=500, \n",
    "                          class_weight=train_class_weights,\n",
    "                          validation_data=val_batches,\n",
    "                          callbacks=[early]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 13s 658ms/step\n",
      "F1 score = 0.3176373743259617\n",
      "Balanced accuracy = 0.33648567119155354\n",
      "Relative confusion matrix:\n",
      "[[0.         0.22       0.78      ]\n",
      " [0.         0.26176471 0.73823529]\n",
      " [0.         0.25230769 0.74769231]]\n"
     ]
    }
   ],
   "source": [
    "# Predicao no conjunto de validacao\n",
    "y_val_pred = model_ResNet50_frozen.predict(val_batches)\n",
    "\n",
    "# Metricas\n",
    "print('F1 score = '+str(f1_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),average='macro')))\n",
    "print('Balanced accuracy = '+str(balanced_accuracy_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1))))\n",
    "print('Relative confusion matrix:')\n",
    "print(confusion_matrix(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),normalize='true'))\n",
    "\n",
    "# Salva modelo\n",
    "model_ResNet50_frozen.save(PATH_TO_WEIGHTS + 'ResNet50_frozen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de parametros treinaveis: 1060867\n",
      "Epoch 1/500\n",
      "69/69 [==============================] - 147s 2s/step - loss: 5.1783 - accuracy: 0.3456 - val_loss: 16.0660 - val_accuracy: 0.0917\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 4.2593 - accuracy: 0.3931 - val_loss: 8.4782 - val_accuracy: 0.5963\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 134s 2s/step - loss: 5.8203 - accuracy: 0.3919 - val_loss: 4.7321 - val_accuracy: 0.5963\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 6.5642 - accuracy: 0.3440 - val_loss: 7.6716 - val_accuracy: 0.1174\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 134s 2s/step - loss: 6.1901 - accuracy: 0.3564 - val_loss: 1.5360 - val_accuracy: 0.3963\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 3.2660 - accuracy: 0.4864 - val_loss: 8.8434 - val_accuracy: 0.0972\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 4.3948 - accuracy: 0.3190 - val_loss: 5.7388 - val_accuracy: 0.2844\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 134s 2s/step - loss: 4.8857 - accuracy: 0.4378 - val_loss: 14.5096 - val_accuracy: 0.1541\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 3.1706 - accuracy: 0.4144 - val_loss: 2.5400 - val_accuracy: 0.5945\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 3.0877 - accuracy: 0.4020 - val_loss: 9.9826 - val_accuracy: 0.0917\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 4.8121 - accuracy: 0.2881 - val_loss: 6.0602 - val_accuracy: 0.1358\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 2.7580 - accuracy: 0.3582 - val_loss: 2.1603 - val_accuracy: 0.4028\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 2.3494 - accuracy: 0.4348 - val_loss: 1.9185 - val_accuracy: 0.3385\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 2.2363 - accuracy: 0.4889 - val_loss: 4.4076 - val_accuracy: 0.3697\n",
      "Epoch 15/500\n",
      "69/69 [==============================] - 133s 2s/step - loss: 2.0482 - accuracy: 0.4909 - val_loss: 3.9441 - val_accuracy: 0.3147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d571a7c70>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Carregamento do modelo pré-treinado sem as camadas densas\n",
    "model_ResNet152 = tf.keras.applications.resnet.ResNet152(weights='imagenet', include_top=False, input_shape=(width,height,3))\n",
    "# Cria modelo para posterior fine tunning\n",
    "model_ResNet152_frozen = define_frozen_network(model_ResNet152, 4, n_classes)\n",
    "\n",
    "# Treina modelo\n",
    "model_ResNet152_frozen.fit(train_batches, \n",
    "                           epochs=500, \n",
    "                           class_weight=train_class_weights,\n",
    "                           validation_data=val_batches,\n",
    "                           callbacks=[early]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 26s 1s/step\n",
      "F1 score = 0.28043826716922216\n",
      "Balanced accuracy = 0.3159728506787331\n",
      "Relative confusion matrix:\n",
      "[[0.03       0.53       0.44      ]\n",
      " [0.00294118 0.61176471 0.38529412]\n",
      " [0.00461538 0.68923077 0.30615385]]\n"
     ]
    }
   ],
   "source": [
    "# Predicao no conjunto de validacao\n",
    "y_val_pred = model_ResNet152_frozen.predict(val_batches)\n",
    "\n",
    "# Metricas\n",
    "print('F1 score = '+str(f1_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),average='macro')))\n",
    "print('Balanced accuracy = '+str(balanced_accuracy_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1))))\n",
    "print('Relative confusion matrix:')\n",
    "print(confusion_matrix(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),normalize='true'))\n",
    "\n",
    "# Salva modelo\n",
    "model_ResNet152_frozen.save(PATH_TO_WEIGHTS + 'ResNet152_frozen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 13s 0us/step\n",
      "Numero de parametros treinaveis: 6147\n",
      "Epoch 1/500\n",
      "69/69 [==============================] - 21s 274ms/step - loss: 4484.1797 - accuracy: 0.4398 - val_loss: 15044.8115 - val_accuracy: 0.3119\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 19s 281ms/step - loss: 5347.6816 - accuracy: 0.2741 - val_loss: 8648.7920 - val_accuracy: 0.0917\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 19s 279ms/step - loss: 4621.1626 - accuracy: 0.3401 - val_loss: 1052.9166 - val_accuracy: 0.1394\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 20s 284ms/step - loss: 4101.3296 - accuracy: 0.2879 - val_loss: 911.6999 - val_accuracy: 0.5945\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 20s 295ms/step - loss: 5550.9326 - accuracy: 0.3463 - val_loss: 1103.6455 - val_accuracy: 0.3193\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 20s 294ms/step - loss: 5295.9551 - accuracy: 0.3440 - val_loss: 1908.8781 - val_accuracy: 0.5963\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 20s 293ms/step - loss: 3592.6663 - accuracy: 0.3126 - val_loss: 7719.1128 - val_accuracy: 0.0917\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 20s 290ms/step - loss: 2820.4873 - accuracy: 0.3264 - val_loss: 943.2827 - val_accuracy: 0.5349\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 20s 290ms/step - loss: 4441.7734 - accuracy: 0.3649 - val_loss: 2214.0735 - val_accuracy: 0.3202\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 20s 290ms/step - loss: 4419.1123 - accuracy: 0.4155 - val_loss: 1341.7548 - val_accuracy: 0.5963\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 20s 289ms/step - loss: 2480.9131 - accuracy: 0.3012 - val_loss: 1449.9489 - val_accuracy: 0.3119\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 20s 289ms/step - loss: 2979.0662 - accuracy: 0.3640 - val_loss: 1314.5924 - val_accuracy: 0.5514\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 20s 292ms/step - loss: 4150.1011 - accuracy: 0.2274 - val_loss: 1983.5880 - val_accuracy: 0.3119\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 20s 290ms/step - loss: 4566.6743 - accuracy: 0.2604 - val_loss: 1163.8846 - val_accuracy: 0.1752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d4c296790>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Carregamento do modelo pré-treinado sem as camadas densas\n",
    "model_InceptionV3 = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(width,height,3))\n",
    "# Cria modelo para posterior fine tunning\n",
    "model_InceptionV3_frozen = define_frozen_network(model_InceptionV3, 4, n_classes)\n",
    "\n",
    "# Treina modelo\n",
    "model_InceptionV3_frozen.fit(train_batches, \n",
    "                             epochs=500, \n",
    "                             class_weight=train_class_weights,\n",
    "                             validation_data=val_batches,\n",
    "                             callbacks=[early]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 200ms/step\n",
      "F1 score = 0.24918535556833432\n",
      "Balanced accuracy = 0.3333333333333333\n",
      "Relative confusion matrix:\n",
      "[[0.         0.         1.        ]\n",
      " [0.00294118 0.         0.99705882]\n",
      " [0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Predicao no conjunto de validacao\n",
    "y_val_pred = model_InceptionV3_frozen.predict(val_batches)\n",
    "\n",
    "# Metricas\n",
    "print('F1 score = '+str(f1_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),average='macro')))\n",
    "print('Balanced accuracy = '+str(balanced_accuracy_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1))))\n",
    "print('Relative confusion matrix:')\n",
    "print(confusion_matrix(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),normalize='true'))\n",
    "\n",
    "# Salva modelo\n",
    "model_InceptionV3_frozen.save(PATH_TO_WEIGHTS + 'InceptionV3_frozen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de parametros treinaveis: 2361347\n",
      "Epoch 1/500\n",
      "69/69 [==============================] - 56s 807ms/step - loss: 7.1670 - accuracy: 0.2329 - val_loss: 1.1037 - val_accuracy: 0.4596\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 55s 803ms/step - loss: 1.0694 - accuracy: 0.3780 - val_loss: 1.1307 - val_accuracy: 0.2505\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 54s 788ms/step - loss: 1.0222 - accuracy: 0.5072 - val_loss: 1.1182 - val_accuracy: 0.4321\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 55s 796ms/step - loss: 1.0428 - accuracy: 0.5042 - val_loss: 1.1118 - val_accuracy: 0.4367\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.9493 - accuracy: 0.4566 - val_loss: 1.1216 - val_accuracy: 0.4266\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 55s 794ms/step - loss: 1.0498 - accuracy: 0.5267 - val_loss: 1.1120 - val_accuracy: 0.5138\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 57s 824ms/step - loss: 1.0141 - accuracy: 0.5036 - val_loss: 1.0905 - val_accuracy: 0.4826\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 60s 870ms/step - loss: 0.8919 - accuracy: 0.5625 - val_loss: 1.1539 - val_accuracy: 0.4211\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.8260 - accuracy: 0.5966 - val_loss: 1.1653 - val_accuracy: 0.4550\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 56s 812ms/step - loss: 0.8726 - accuracy: 0.5929 - val_loss: 1.1143 - val_accuracy: 0.5220\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.7850 - accuracy: 0.5620 - val_loss: 1.1305 - val_accuracy: 0.4963\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 57s 834ms/step - loss: 0.8148 - accuracy: 0.6209 - val_loss: 1.3715 - val_accuracy: 0.3119\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.7305 - accuracy: 0.6039 - val_loss: 1.1549 - val_accuracy: 0.5028\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 58s 843ms/step - loss: 0.7463 - accuracy: 0.6489 - val_loss: 1.3122 - val_accuracy: 0.3596\n",
      "Epoch 15/500\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.7717 - accuracy: 0.5959 - val_loss: 1.2295 - val_accuracy: 0.3881\n",
      "Epoch 16/500\n",
      "69/69 [==============================] - 57s 832ms/step - loss: 0.6945 - accuracy: 0.6521 - val_loss: 1.4350 - val_accuracy: 0.3147\n",
      "Epoch 17/500\n",
      "69/69 [==============================] - 57s 824ms/step - loss: 0.6823 - accuracy: 0.6424 - val_loss: 1.1751 - val_accuracy: 0.4734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d524de8e0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Carregamento do modelo pré-treinado sem as camadas densas\n",
    "model_VGG16 = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(width,height,3))\n",
    "# Cria modelo para posterior fine tunning\n",
    "model_VGG16_frozen = define_frozen_network(model_VGG16, 2, n_classes)\n",
    "\n",
    "# Treina modelo\n",
    "model_VGG16_frozen.fit(train_batches,\n",
    "                       epochs=500, \n",
    "                       class_weight=train_class_weights,\n",
    "                       validation_data=val_batches,\n",
    "                       callbacks=[early]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 19s 1s/step\n",
      "F1 score = 0.3290318686404194\n",
      "Balanced accuracy = 0.33057315233785817\n",
      "Relative confusion matrix:\n",
      "[[0.06       0.35       0.59      ]\n",
      " [0.06764706 0.27941176 0.65294118]\n",
      " [0.03538462 0.31230769 0.65230769]]\n"
     ]
    }
   ],
   "source": [
    "# Predicao no conjunto de validacao\n",
    "y_val_pred = model_VGG16_frozen.predict(val_batches)\n",
    "\n",
    "# Metricas\n",
    "print('F1 score = '+str(f1_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),average='macro')))\n",
    "print('Balanced accuracy = '+str(balanced_accuracy_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1))))\n",
    "print('Relative confusion matrix:')\n",
    "print(confusion_matrix(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),normalize='true'))\n",
    "\n",
    "model_VGG16_frozen.save(PATH_TO_WEIGHTS + 'VGG16_frozen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 10s 0us/step\n",
      "Numero de parametros treinaveis: 2361347\n",
      "Epoch 1/500\n",
      "69/69 [==============================] - 68s 976ms/step - loss: 12.6872 - accuracy: 0.2661 - val_loss: 1.1306 - val_accuracy: 0.1725\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 69s 997ms/step - loss: 1.0684 - accuracy: 0.3514 - val_loss: 1.1441 - val_accuracy: 0.1853\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 69s 1s/step - loss: 1.0316 - accuracy: 0.2815 - val_loss: 1.1133 - val_accuracy: 0.2633\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 71s 1s/step - loss: 1.0301 - accuracy: 0.4994 - val_loss: 1.1034 - val_accuracy: 0.4661\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 68s 986ms/step - loss: 0.9979 - accuracy: 0.4288 - val_loss: 1.0889 - val_accuracy: 0.4991\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 69s 999ms/step - loss: 0.9792 - accuracy: 0.5501 - val_loss: 1.1326 - val_accuracy: 0.4128\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 68s 980ms/step - loss: 0.9629 - accuracy: 0.5625 - val_loss: 1.1286 - val_accuracy: 0.4743\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 67s 973ms/step - loss: 0.9308 - accuracy: 0.5402 - val_loss: 1.1498 - val_accuracy: 0.4165\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 67s 977ms/step - loss: 1.0406 - accuracy: 0.5228 - val_loss: 1.1164 - val_accuracy: 0.4569\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 67s 974ms/step - loss: 0.9331 - accuracy: 0.5686 - val_loss: 1.1764 - val_accuracy: 0.4367\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.8154 - accuracy: 0.5638 - val_loss: 1.2682 - val_accuracy: 0.5890\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.8236 - accuracy: 0.5730 - val_loss: 1.1436 - val_accuracy: 0.4385\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 64s 934ms/step - loss: 0.7719 - accuracy: 0.6335 - val_loss: 1.2779 - val_accuracy: 0.3945\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 64s 929ms/step - loss: 0.8115 - accuracy: 0.5945 - val_loss: 1.2057 - val_accuracy: 0.4294\n",
      "Epoch 15/500\n",
      "69/69 [==============================] - 65s 939ms/step - loss: 0.7802 - accuracy: 0.6076 - val_loss: 1.1473 - val_accuracy: 0.4780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d586e76d0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Carregamento do modelo pré-treinado sem as camadas densas\n",
    "model_VGG19 = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(width,height,3))\n",
    "# Cria modelo para posterior fine tunning\n",
    "model_VGG19_frozen = define_frozen_network(model_VGG19, 2, n_classes)\n",
    "\n",
    "# Treina modelo\n",
    "model_VGG19_frozen.fit(train_batches,\n",
    "                       epochs=500, \n",
    "                       class_weight=train_class_weights,\n",
    "                       validation_data=val_batches,\n",
    "                       callbacks=[early]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 23s 1s/step\n",
      "F1 score = 0.3091620032937992\n",
      "Balanced accuracy = 0.31576168929110104\n",
      "Relative confusion matrix:\n",
      "[[0.02       0.28       0.7       ]\n",
      " [0.09117647 0.20882353 0.7       ]\n",
      " [0.08615385 0.19538462 0.71846154]]\n"
     ]
    }
   ],
   "source": [
    "# Predicao no conjunto de validacao\n",
    "y_val_pred = model_VGG19_frozen.predict(val_batches)\n",
    "\n",
    "# Metricas\n",
    "print('F1 score = '+str(f1_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),average='macro')))\n",
    "print('Balanced accuracy = '+str(balanced_accuracy_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1))))\n",
    "print('Relative confusion matrix:')\n",
    "print(confusion_matrix(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),normalize='true'))\n",
    "\n",
    "model_VGG19_frozen.save(PATH_TO_WEIGHTS + 'VGG19_frozen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetV2B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de parametros treinaveis: 252163\n",
      "Epoch 1/500\n",
      "69/69 [==============================] - 22s 264ms/step - loss: 1.2141 - accuracy: 0.2812 - val_loss: 1.1164 - val_accuracy: 0.3193\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 17s 244ms/step - loss: 1.2018 - accuracy: 0.2948 - val_loss: 1.1246 - val_accuracy: 0.3009\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 16s 238ms/step - loss: 1.1684 - accuracy: 0.2659 - val_loss: 1.1351 - val_accuracy: 0.3018\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 17s 241ms/step - loss: 1.1530 - accuracy: 0.3035 - val_loss: 1.1921 - val_accuracy: 0.2908\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 17s 245ms/step - loss: 1.1458 - accuracy: 0.2569 - val_loss: 1.1472 - val_accuracy: 0.2927\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 17s 241ms/step - loss: 1.1337 - accuracy: 0.2732 - val_loss: 1.1247 - val_accuracy: 0.2853\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 16s 237ms/step - loss: 1.1345 - accuracy: 0.2766 - val_loss: 1.1396 - val_accuracy: 0.2817\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 17s 240ms/step - loss: 1.1298 - accuracy: 0.3055 - val_loss: 1.2009 - val_accuracy: 0.2596\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 17s 244ms/step - loss: 1.1286 - accuracy: 0.2817 - val_loss: 1.1361 - val_accuracy: 0.2872\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 16s 237ms/step - loss: 1.1250 - accuracy: 0.2966 - val_loss: 1.1787 - val_accuracy: 0.2578\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 17s 242ms/step - loss: 1.1204 - accuracy: 0.2535 - val_loss: 1.1166 - val_accuracy: 0.2890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b6bf1eabe0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Carregamento do modelo pré-treinado sem as camadas densas\n",
    "model_EfficientNetV2B0 = tf.keras.applications.EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(width,height,3))\n",
    "# Cria modelo para posterior fine tunning\n",
    "model_EfficientNetV2B0_frozen = define_frozen_network(model_EfficientNetV2B0, 4, n_classes)\n",
    "\n",
    "# Treina modelo\n",
    "model_EfficientNetV2B0_frozen.fit(train_batches,\n",
    "                                  epochs=500, \n",
    "                                  class_weight=train_class_weights,\n",
    "                                  validation_data=val_batches,\n",
    "                                  callbacks=[early]\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 182ms/step\n",
      "F1 score = 0.2987120869172089\n",
      "Balanced accuracy = 0.3540874811463047\n",
      "Relative confusion matrix:\n",
      "[[0.44       0.21       0.35      ]\n",
      " [0.38529412 0.31764706 0.29705882]\n",
      " [0.41846154 0.27692308 0.30461538]]\n"
     ]
    }
   ],
   "source": [
    "# Predicao no conjunto de validacao\n",
    "y_val_pred = model_EfficientNetV2B0_frozen.predict(val_batches)\n",
    "\n",
    "# Metricas\n",
    "print('F1 score = '+str(f1_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),average='macro')))\n",
    "print('Balanced accuracy = '+str(balanced_accuracy_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1))))\n",
    "print('Relative confusion matrix:')\n",
    "print(confusion_matrix(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),normalize='true'))\n",
    "\n",
    "model_EfficientNetV2B0_frozen.save(PATH_TO_WEIGHTS + 'EfficientNetV2B0_frozen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetV2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-m_notop.h5\n",
      "214201816/214201816 [==============================] - 33s 0us/step\n",
      "Numero de parametros treinaveis: 661763\n",
      "Epoch 1/500\n",
      "69/69 [==============================] - 104s 1s/step - loss: 1.1633 - accuracy: 0.3216 - val_loss: 1.1177 - val_accuracy: 0.3596\n",
      "Epoch 2/500\n",
      "69/69 [==============================] - 89s 1s/step - loss: 1.1624 - accuracy: 0.3397 - val_loss: 1.1747 - val_accuracy: 0.2679\n",
      "Epoch 3/500\n",
      "69/69 [==============================] - 91s 1s/step - loss: 1.1360 - accuracy: 0.2993 - val_loss: 1.1035 - val_accuracy: 0.3688\n",
      "Epoch 4/500\n",
      "69/69 [==============================] - 93s 1s/step - loss: 1.1422 - accuracy: 0.2821 - val_loss: 1.0856 - val_accuracy: 0.3789\n",
      "Epoch 5/500\n",
      "69/69 [==============================] - 94s 1s/step - loss: 1.1534 - accuracy: 0.3076 - val_loss: 1.0826 - val_accuracy: 0.3761\n",
      "Epoch 6/500\n",
      "69/69 [==============================] - 92s 1s/step - loss: 1.1377 - accuracy: 0.3365 - val_loss: 1.1318 - val_accuracy: 0.3028\n",
      "Epoch 7/500\n",
      "69/69 [==============================] - 90s 1s/step - loss: 1.1238 - accuracy: 0.3351 - val_loss: 1.1258 - val_accuracy: 0.3284\n",
      "Epoch 8/500\n",
      "69/69 [==============================] - 87s 1s/step - loss: 1.1259 - accuracy: 0.3280 - val_loss: 1.1086 - val_accuracy: 0.3523\n",
      "Epoch 9/500\n",
      "69/69 [==============================] - 88s 1s/step - loss: 1.1232 - accuracy: 0.2771 - val_loss: 1.1086 - val_accuracy: 0.3486\n",
      "Epoch 10/500\n",
      "69/69 [==============================] - 89s 1s/step - loss: 1.1241 - accuracy: 0.3397 - val_loss: 1.1025 - val_accuracy: 0.3422\n",
      "Epoch 11/500\n",
      "69/69 [==============================] - 87s 1s/step - loss: 1.1253 - accuracy: 0.3805 - val_loss: 1.0914 - val_accuracy: 0.3670\n",
      "Epoch 12/500\n",
      "69/69 [==============================] - 87s 1s/step - loss: 1.1142 - accuracy: 0.3303 - val_loss: 1.1287 - val_accuracy: 0.3101\n",
      "Epoch 13/500\n",
      "69/69 [==============================] - 87s 1s/step - loss: 1.1147 - accuracy: 0.3170 - val_loss: 1.1204 - val_accuracy: 0.3239\n",
      "Epoch 14/500\n",
      "69/69 [==============================] - 88s 1s/step - loss: 1.1194 - accuracy: 0.3592 - val_loss: 1.1052 - val_accuracy: 0.3404\n",
      "Epoch 15/500\n",
      "69/69 [==============================] - 88s 1s/step - loss: 1.1193 - accuracy: 0.3236 - val_loss: 1.1360 - val_accuracy: 0.3083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d5708bfd0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "early =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Carregamento do modelo pré-treinado sem as camadas densas\n",
    "model_EfficientNetV2M = tf.keras.applications.EfficientNetV2M(weights='imagenet', include_top=False, input_shape=(width,height,3))\n",
    "# Cria modelo para posterior fine tunning\n",
    "model_EfficientNetV2M_frozen = define_frozen_network(model_EfficientNetV2M, 4, n_classes)\n",
    "\n",
    "# Treina modelo\n",
    "model_EfficientNetV2M_frozen.fit(train_batches,\n",
    "                                 epochs=500, \n",
    "                                 class_weight=train_class_weights,\n",
    "                                 validation_data=val_batches,\n",
    "                                 callbacks=[early]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 19s 1s/step\n",
      "F1 score = 0.30587029068635646\n",
      "Balanced accuracy = 0.31874811463046754\n",
      "Relative confusion matrix:\n",
      "[[0.22       0.4        0.38      ]\n",
      " [0.25294118 0.31470588 0.43235294]\n",
      " [0.23846154 0.34       0.42153846]]\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Predicao no conjunto de validacao\n",
    "y_val_pred = model_EfficientNetV2M_frozen.predict(val_batches)\n",
    "\n",
    "# Metricas\n",
    "print('F1 score = '+str(f1_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),average='macro')))\n",
    "print('Balanced accuracy = '+str(balanced_accuracy_score(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1))))\n",
    "print('Relative confusion matrix:')\n",
    "print(confusion_matrix(val_batches.y.argmax(axis=-1),y_val_pred.argmax(axis=-1),normalize='true'))\n",
    "\n",
    "model_EfficientNetV2M_frozen.save(PATH_TO_WEIGHTS + 'EfficientNetV2M_frozen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilação dos resultados\n",
    "\n",
    "Em termos de acurácia normalizada, as melhores arquiteturas foram:\n",
    "1. EfficientNetV2B0 = 34.63%\n",
    "2. ResNet50 = 33.65%\n",
    "3. InceptionV3 = 33.33%\n",
    "4. VGG16 = 33.06%\n",
    "5. EfficientNetV2M = 31.87%\n",
    "6. ResNet152 = 31.60%\n",
    "7. VGG19 = 31.58%\n",
    "\n",
    "No geral, nenhuma das redes cons"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "analise_sentimento_memes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2487189b5835e73c5fa6cd7f2ed518f6fa1ff6c9bd8da44d17cb8ab10f576c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
